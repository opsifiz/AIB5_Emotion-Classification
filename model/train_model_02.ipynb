{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12108731,"sourceType":"datasetVersion","datasetId":7623617}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets scikit-learn evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:42:44.902216Z","iopub.execute_input":"2025-06-09T13:42:44.902573Z","iopub.status.idle":"2025-06-09T13:42:50.460689Z","shell.execute_reply.started":"2025-06-09T13:42:44.902523Z","shell.execute_reply":"2025-06-09T13:42:50.459958Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:42:50.462315Z","iopub.execute_input":"2025-06-09T13:42:50.462654Z","iopub.status.idle":"2025-06-09T13:42:50.757237Z","shell.execute_reply.started":"2025-06-09T13:42:50.462620Z","shell.execute_reply":"2025-06-09T13:42:50.756439Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/train-dataset2-csv/train_dataset2.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:43:14.182680Z","iopub.execute_input":"2025-06-09T13:43:14.183014Z","iopub.status.idle":"2025-06-09T13:43:15.784454Z","shell.execute_reply.started":"2025-06-09T13:43:14.182991Z","shell.execute_reply":"2025-06-09T13:43:15.783610Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                    text   status\n0                                             oh my gosh  Anxiety\n1      trouble sleeping confused mind restless heart ...  Anxiety\n2      all wrong back off dear forward doubt stay in ...  Anxiety\n3      i have shifted my focus to something else but ...  Anxiety\n4      i am restless and restless it is been a month ...  Anxiety\n...                                                  ...      ...\n89995  tw strong arm abuse my dad was screaming at me...   Stress\n89996  hi i cannot think clearly today i know i have ...   Stress\n89997  my chest give a dissimilar aroma before it wou...   Stress\n89998  he was going to choke the name out of me i am ...   Stress\n89999  hurt the ego but whatever i speculation you ar...   Stress\n\n[90000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>oh my gosh</td>\n      <td>Anxiety</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>trouble sleeping confused mind restless heart ...</td>\n      <td>Anxiety</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>all wrong back off dear forward doubt stay in ...</td>\n      <td>Anxiety</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i have shifted my focus to something else but ...</td>\n      <td>Anxiety</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am restless and restless it is been a month ...</td>\n      <td>Anxiety</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>89995</th>\n      <td>tw strong arm abuse my dad was screaming at me...</td>\n      <td>Stress</td>\n    </tr>\n    <tr>\n      <th>89996</th>\n      <td>hi i cannot think clearly today i know i have ...</td>\n      <td>Stress</td>\n    </tr>\n    <tr>\n      <th>89997</th>\n      <td>my chest give a dissimilar aroma before it wou...</td>\n      <td>Stress</td>\n    </tr>\n    <tr>\n      <th>89998</th>\n      <td>he was going to choke the name out of me i am ...</td>\n      <td>Stress</td>\n    </tr>\n    <tr>\n      <th>89999</th>\n      <td>hurt the ego but whatever i speculation you ar...</td>\n      <td>Stress</td>\n    </tr>\n  </tbody>\n</table>\n<p>90000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Load and encode labels\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['status'])\n\n# Get label names and mapping\nlabel_names = label_encoder.classes_\nnum_labels = len(label_names)\n\n# Optional: Check label mappings\nlabel_mapping = dict(zip(label_names, range(num_labels)))\nprint(label_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:43:19.474160Z","iopub.execute_input":"2025-06-09T13:43:19.474435Z","iopub.status.idle":"2025-06-09T13:43:20.044106Z","shell.execute_reply.started":"2025-06-09T13:43:19.474414Z","shell.execute_reply":"2025-06-09T13:43:20.043382Z"}},"outputs":[{"name":"stdout","text":"{'Anxiety': 0, 'BPD': 1, 'Normal': 2, 'Stress': 3, 'Suicidal': 4, 'bipolar': 5, 'depression': 6, 'mentalillness': 7, 'schizophrenia': 8}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset\n\n# Convert to Hugging Face Dataset format\ndataset = Dataset.from_pandas(df[['text', 'label']])\ndataset = dataset.train_test_split(test_size=0.2)  # Split into train/test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:43:21.503252Z","iopub.execute_input":"2025-06-09T13:43:21.503997Z","iopub.status.idle":"2025-06-09T13:43:23.605938Z","shell.execute_reply.started":"2025-06-09T13:43:21.503973Z","shell.execute_reply":"2025-06-09T13:43:23.605139Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"distilbert-base-uncased\"  # Use bart-base for faster fine-tuning than bart-large-mnli\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:44:22.682774Z","iopub.execute_input":"2025-06-09T13:44:22.683471Z","iopub.status.idle":"2025-06-09T13:44:22.794816Z","shell.execute_reply.started":"2025-06-09T13:44:22.683448Z","shell.execute_reply":"2025-06-09T13:44:22.794097Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.006,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:44:25.675946Z","iopub.execute_input":"2025-06-09T13:44:25.676140Z","iopub.status.idle":"2025-06-09T13:44:26.009160Z","shell.execute_reply.started":"2025-06-09T13:44:25.676125Z","shell.execute_reply":"2025-06-09T13:44:26.008365Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1637027611.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:44:35.994035Z","iopub.execute_input":"2025-06-09T13:44:35.994672Z","iopub.status.idle":"2025-06-09T14:47:12.227216Z","shell.execute_reply.started":"2025-06-09T13:44:35.994647Z","shell.execute_reply":"2025-06-09T14:47:12.226514Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13500/13500 1:02:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.287200</td>\n      <td>1.174998</td>\n      <td>0.609278</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.265800</td>\n      <td>1.091090</td>\n      <td>0.634889</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.929900</td>\n      <td>1.039384</td>\n      <td>0.649389</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.214800</td>\n      <td>1.001145</td>\n      <td>0.662000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.046100</td>\n      <td>0.960062</td>\n      <td>0.675056</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.892900</td>\n      <td>0.946662</td>\n      <td>0.679611</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.037800</td>\n      <td>0.942225</td>\n      <td>0.679389</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.017700</td>\n      <td>0.925471</td>\n      <td>0.678722</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.919200</td>\n      <td>0.909515</td>\n      <td>0.688000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.789400</td>\n      <td>0.928519</td>\n      <td>0.689833</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.813900</td>\n      <td>0.924350</td>\n      <td>0.694167</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.883600</td>\n      <td>0.902113</td>\n      <td>0.692889</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.863800</td>\n      <td>0.915249</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.709100</td>\n      <td>0.924967</td>\n      <td>0.694278</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.760800</td>\n      <td>0.892682</td>\n      <td>0.694611</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.692600</td>\n      <td>0.878841</td>\n      <td>0.700556</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.835100</td>\n      <td>0.880902</td>\n      <td>0.699444</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.674200</td>\n      <td>0.863896</td>\n      <td>0.700667</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.535500</td>\n      <td>0.941179</td>\n      <td>0.701778</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.437900</td>\n      <td>0.951923</td>\n      <td>0.695833</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.511600</td>\n      <td>0.959744</td>\n      <td>0.696500</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.535700</td>\n      <td>0.944093</td>\n      <td>0.700667</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.472000</td>\n      <td>0.962223</td>\n      <td>0.702389</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.570700</td>\n      <td>0.950873</td>\n      <td>0.699222</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.402600</td>\n      <td>0.949359</td>\n      <td>0.702333</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.405100</td>\n      <td>0.950836</td>\n      <td>0.702167</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.508700</td>\n      <td>0.946610</td>\n      <td>0.701722</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=13500, training_loss=0.7952181755348489, metrics={'train_runtime': 3755.7947, 'train_samples_per_second': 57.511, 'train_steps_per_second': 3.594, 'total_flos': 7154132502528000.0, 'train_loss': 0.7952181755348489, 'epoch': 3.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npredictions = trainer.predict(tokenized_datasets[\"test\"])  # or [\"validation\"]\npreds = predictions.predictions.argmax(axis=-1)\nlabels = predictions.label_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T14:48:43.708469Z","iopub.execute_input":"2025-06-09T14:48:43.708781Z","iopub.status.idle":"2025-06-09T14:49:43.600866Z","shell.execute_reply.started":"2025-06-09T14:48:43.708757Z","shell.execute_reply":"2025-06-09T14:49:43.600064Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"print(classification_report(labels, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T14:49:43.602235Z","iopub.execute_input":"2025-06-09T14:49:43.602477Z","iopub.status.idle":"2025-06-09T14:49:43.622505Z","shell.execute_reply.started":"2025-06-09T14:49:43.602460Z","shell.execute_reply":"2025-06-09T14:49:43.621923Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.75      0.77      0.76      2102\n           1       0.67      0.69      0.68      1999\n           2       0.88      0.89      0.88      1995\n           3       0.90      0.89      0.89      1998\n           4       0.66      0.65      0.66      2003\n           5       0.69      0.70      0.69      1980\n           6       0.57      0.53      0.55      2032\n           7       0.47      0.48      0.47      1937\n           8       0.71      0.71      0.71      1954\n\n    accuracy                           0.70     18000\n   macro avg       0.70      0.70      0.70     18000\nweighted avg       0.70      0.70      0.70     18000\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.save_pretrained(\"./my_model\")\ntokenizer.save_pretrained(\"./my_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T14:49:43.623114Z","iopub.execute_input":"2025-06-09T14:49:43.623305Z","iopub.status.idle":"2025-06-09T14:49:44.233967Z","shell.execute_reply.started":"2025-06-09T14:49:43.623290Z","shell.execute_reply":"2025-06-09T14:49:44.233356Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('./my_model/tokenizer_config.json',\n './my_model/special_tokens_map.json',\n './my_model/vocab.txt',\n './my_model/added_tokens.json',\n './my_model/tokenizer.json')"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"!zip -r my_model.zip my_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T14:49:44.235513Z","iopub.execute_input":"2025-06-09T14:49:44.235798Z","iopub.status.idle":"2025-06-09T14:49:59.467509Z","shell.execute_reply.started":"2025-06-09T14:49:44.235780Z","shell.execute_reply":"2025-06-09T14:49:59.466707Z"}},"outputs":[{"name":"stdout","text":"  adding: my_model/ (stored 0%)\n  adding: my_model/config.json (deflated 55%)\n  adding: my_model/tokenizer_config.json (deflated 75%)\n  adding: my_model/vocab.txt (deflated 53%)\n  adding: my_model/special_tokens_map.json (deflated 42%)\n  adding: my_model/model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 8%)\n  adding: my_model/tokenizer.json (deflated 71%)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('my_model', 'zip', '/kaggle/working/my_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T14:49:59.468520Z","iopub.execute_input":"2025-06-09T14:49:59.468789Z","iopub.status.idle":"2025-06-09T14:50:14.213523Z","shell.execute_reply.started":"2025-06-09T14:49:59.468765Z","shell.execute_reply":"2025-06-09T14:50:14.212750Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/my_model.zip'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'my_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T14:50:14.214858Z","iopub.execute_input":"2025-06-09T14:50:14.215152Z","iopub.status.idle":"2025-06-09T14:50:14.220420Z","shell.execute_reply.started":"2025-06-09T14:50:14.215129Z","shell.execute_reply":"2025-06-09T14:50:14.219880Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/my_model.zip","text/html":"<a href='my_model.zip' target='_blank'>my_model.zip</a><br>"},"metadata":{}}],"execution_count":19}]}
