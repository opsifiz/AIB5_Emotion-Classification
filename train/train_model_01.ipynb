{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12061639,"sourceType":"datasetVersion","datasetId":7591875}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets scikit-learn evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:06:48.571084Z","iopub.execute_input":"2025-06-05T08:06:48.571380Z","iopub.status.idle":"2025-06-05T08:06:53.381903Z","shell.execute_reply.started":"2025-06-05T08:06:48.571359Z","shell.execute_reply":"2025-06-05T08:06:53.381182Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:06:57.498214Z","iopub.execute_input":"2025-06-05T08:06:57.498854Z","iopub.status.idle":"2025-06-05T08:06:57.502740Z","shell.execute_reply.started":"2025-06-05T08:06:57.498824Z","shell.execute_reply":"2025-06-05T08:06:57.501994Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dataset/train_dataset.csv')\ndf.drop(columns=['Unnamed: 0'], inplace=True)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:07:00.008653Z","iopub.execute_input":"2025-06-05T08:07:00.009372Z","iopub.status.idle":"2025-06-05T08:07:01.948542Z","shell.execute_reply.started":"2025-06-05T08:07:00.009337Z","shell.execute_reply":"2025-06-05T08:07:01.947701Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                     text         status from  \\\n0                                              oh my gosh        Anxiety  df1   \n1       trouble sleeping confused mind restless heart ...        Anxiety  df1   \n2       all wrong back off dear forward doubt stay in ...        Anxiety  df1   \n3       i have shifted my focus to something else but ...        Anxiety  df1   \n4       i am restless and restless it is been a month ...        Anxiety  df1   \n...                                                   ...            ...  ...   \n104995  low testosterone after discontinuing rispredon...  schizophrenia  df3   \n104996  how did you finally accept your diagnosis i am...  schizophrenia  df3   \n104997  constantly feel like i am in a competition wit...  schizophrenia  df3   \n104998  has anyone switched over to an entirely differ...  schizophrenia  df3   \n104999  has anyone experienced anything like this psyc...  schizophrenia  df3   \n\n                                               translated  \n0                                             โอ้พระเจ้า!  \n1       นอนไม่หลับ วุ่นวายใจ กระวนกระวายใจ ทุกอย่างดูผ...  \n2       ทุกอย่างไม่ถูกต้อง ถอยไปเถอะ อย่าก้าวไปข้างหน้...  \n3       ฉันพยายามเบนความสนใจไปเรื่องอื่นแล้ว แต่ก็ยังก...  \n4       ฉันกระวนกระวายใจมาก มันเป็นแบบนี้มาเป็นเดือนแล...  \n...                                                   ...  \n104995                                                  -  \n104996                                                  -  \n104997                                                  -  \n104998                                                  -  \n104999                                                  -  \n\n[105000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>status</th>\n      <th>from</th>\n      <th>translated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>oh my gosh</td>\n      <td>Anxiety</td>\n      <td>df1</td>\n      <td>โอ้พระเจ้า!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>trouble sleeping confused mind restless heart ...</td>\n      <td>Anxiety</td>\n      <td>df1</td>\n      <td>นอนไม่หลับ วุ่นวายใจ กระวนกระวายใจ ทุกอย่างดูผ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>all wrong back off dear forward doubt stay in ...</td>\n      <td>Anxiety</td>\n      <td>df1</td>\n      <td>ทุกอย่างไม่ถูกต้อง ถอยไปเถอะ อย่าก้าวไปข้างหน้...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i have shifted my focus to something else but ...</td>\n      <td>Anxiety</td>\n      <td>df1</td>\n      <td>ฉันพยายามเบนความสนใจไปเรื่องอื่นแล้ว แต่ก็ยังก...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am restless and restless it is been a month ...</td>\n      <td>Anxiety</td>\n      <td>df1</td>\n      <td>ฉันกระวนกระวายใจมาก มันเป็นแบบนี้มาเป็นเดือนแล...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104995</th>\n      <td>low testosterone after discontinuing rispredon...</td>\n      <td>schizophrenia</td>\n      <td>df3</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>104996</th>\n      <td>how did you finally accept your diagnosis i am...</td>\n      <td>schizophrenia</td>\n      <td>df3</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>104997</th>\n      <td>constantly feel like i am in a competition wit...</td>\n      <td>schizophrenia</td>\n      <td>df3</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>104998</th>\n      <td>has anyone switched over to an entirely differ...</td>\n      <td>schizophrenia</td>\n      <td>df3</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>104999</th>\n      <td>has anyone experienced anything like this psyc...</td>\n      <td>schizophrenia</td>\n      <td>df3</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n<p>105000 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Load and encode labels\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['status'])\n\n# Get label names and mapping\nlabel_names = label_encoder.classes_\nnum_labels = len(label_names)\n\n# Optional: Check label mappings\nlabel_mapping = dict(zip(label_names, range(num_labels)))\nprint(label_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:07:06.004388Z","iopub.execute_input":"2025-06-05T08:07:06.005094Z","iopub.status.idle":"2025-06-05T08:07:06.028414Z","shell.execute_reply.started":"2025-06-05T08:07:06.005068Z","shell.execute_reply":"2025-06-05T08:07:06.027651Z"}},"outputs":[{"name":"stdout","text":"{'Anxiety': 0, 'BPD': 1, 'Normal': 2, 'bipolar': 3, 'depression': 4, 'mentalillness': 5, 'schizophrenia': 6}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from datasets import Dataset\n\n# Convert to Hugging Face Dataset format\ndataset = Dataset.from_pandas(df[['text', 'label']])\ndataset = dataset.train_test_split(test_size=0.2)  # Split into train/test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:07:09.673076Z","iopub.execute_input":"2025-06-05T08:07:09.674065Z","iopub.status.idle":"2025-06-05T08:07:10.398019Z","shell.execute_reply.started":"2025-06-05T08:07:09.674033Z","shell.execute_reply":"2025-06-05T08:07:10.397477Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"distilbert-base-uncased\"  # Use bart-base for faster fine-tuning than bart-large-mnli\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:26:30.173351Z","iopub.execute_input":"2025-06-05T08:26:30.174188Z","iopub.status.idle":"2025-06-05T08:27:00.578732Z","shell.execute_reply.started":"2025-06-05T08:26:30.174162Z","shell.execute_reply":"2025-06-05T08:27:00.577965Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa30503cdba4e3f9c989476ccb569d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a34c58abc6c4382b8b8ea495dd9ee92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15864afa980744b1ad5027fb34e69564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d5da8a91cf4bc99a250a44fec5e297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/84000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92487472c5344f6db2988abfe7dfe080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f98ca2533046caa6680c2ec3869b57"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:27:00.580021Z","iopub.execute_input":"2025-06-05T08:27:00.580264Z","iopub.status.idle":"2025-06-05T08:27:03.335873Z","shell.execute_reply.started":"2025-06-05T08:27:00.580247Z","shell.execute_reply":"2025-06-05T08:27:03.335344Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f8bd529add422fbd122c6b850cf27d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:27:03.336638Z","iopub.execute_input":"2025-06-05T08:27:03.337378Z","iopub.status.idle":"2025-06-05T08:27:03.747729Z","shell.execute_reply.started":"2025-06-05T08:27:03.337358Z","shell.execute_reply":"2025-06-05T08:27:03.747075Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/2541137833.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:27:03.748803Z","iopub.execute_input":"2025-06-05T08:27:03.749005Z","iopub.status.idle":"2025-06-05T11:15:43.660908Z","shell.execute_reply.started":"2025-06-05T08:27:03.748989Z","shell.execute_reply":"2025-06-05T11:15:43.660178Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31500' max='31500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31500/31500 2:48:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.193400</td>\n      <td>1.121688</td>\n      <td>0.605190</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.901700</td>\n      <td>1.089809</td>\n      <td>0.621571</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.090200</td>\n      <td>1.012352</td>\n      <td>0.649524</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.071400</td>\n      <td>1.006740</td>\n      <td>0.654286</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.918900</td>\n      <td>0.963201</td>\n      <td>0.665238</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.853800</td>\n      <td>0.975199</td>\n      <td>0.667667</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.911800</td>\n      <td>0.922279</td>\n      <td>0.681667</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.894000</td>\n      <td>0.928419</td>\n      <td>0.677238</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.883200</td>\n      <td>0.907786</td>\n      <td>0.682286</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.122800</td>\n      <td>0.940016</td>\n      <td>0.669714</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.831400</td>\n      <td>0.933179</td>\n      <td>0.678714</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.059000</td>\n      <td>0.911564</td>\n      <td>0.688857</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.865500</td>\n      <td>0.891204</td>\n      <td>0.694333</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.794500</td>\n      <td>0.881874</td>\n      <td>0.697381</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.963400</td>\n      <td>0.869173</td>\n      <td>0.697524</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.786100</td>\n      <td>0.869716</td>\n      <td>0.700476</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.982400</td>\n      <td>0.882061</td>\n      <td>0.693286</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.965900</td>\n      <td>0.853100</td>\n      <td>0.704810</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.833500</td>\n      <td>0.875991</td>\n      <td>0.701143</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.726500</td>\n      <td>0.865371</td>\n      <td>0.706000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.995900</td>\n      <td>0.854715</td>\n      <td>0.702190</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.657400</td>\n      <td>0.877730</td>\n      <td>0.707238</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.609700</td>\n      <td>0.863225</td>\n      <td>0.710333</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.698500</td>\n      <td>0.897611</td>\n      <td>0.710238</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.682600</td>\n      <td>0.874812</td>\n      <td>0.714190</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.551800</td>\n      <td>0.874423</td>\n      <td>0.710714</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.892300</td>\n      <td>0.871449</td>\n      <td>0.702857</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.635700</td>\n      <td>0.872317</td>\n      <td>0.712048</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.647200</td>\n      <td>0.869783</td>\n      <td>0.715238</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.840600</td>\n      <td>0.860659</td>\n      <td>0.712143</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.885300</td>\n      <td>0.840787</td>\n      <td>0.710714</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.812100</td>\n      <td>0.866412</td>\n      <td>0.716667</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.668900</td>\n      <td>0.864223</td>\n      <td>0.714571</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.762600</td>\n      <td>0.849108</td>\n      <td>0.716667</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.695800</td>\n      <td>0.840137</td>\n      <td>0.723381</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.706700</td>\n      <td>0.886631</td>\n      <td>0.712571</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.697700</td>\n      <td>0.851269</td>\n      <td>0.720286</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.847200</td>\n      <td>0.846916</td>\n      <td>0.718524</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>1.034400</td>\n      <td>0.837821</td>\n      <td>0.714238</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.513400</td>\n      <td>0.847048</td>\n      <td>0.723190</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.556000</td>\n      <td>0.839384</td>\n      <td>0.722333</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.538900</td>\n      <td>0.836669</td>\n      <td>0.721095</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.523500</td>\n      <td>0.933183</td>\n      <td>0.718619</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.349500</td>\n      <td>0.995145</td>\n      <td>0.714048</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.824900</td>\n      <td>0.966872</td>\n      <td>0.721667</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.667600</td>\n      <td>0.997926</td>\n      <td>0.716857</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.538100</td>\n      <td>0.956964</td>\n      <td>0.718429</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.493100</td>\n      <td>0.968981</td>\n      <td>0.718381</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.393200</td>\n      <td>0.967919</td>\n      <td>0.716190</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.506900</td>\n      <td>0.964394</td>\n      <td>0.719476</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.426900</td>\n      <td>0.978207</td>\n      <td>0.718000</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.445800</td>\n      <td>0.992633</td>\n      <td>0.718524</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.349300</td>\n      <td>1.000792</td>\n      <td>0.720000</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.776600</td>\n      <td>0.997172</td>\n      <td>0.718333</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.539100</td>\n      <td>1.022833</td>\n      <td>0.719286</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.389200</td>\n      <td>0.995422</td>\n      <td>0.720810</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.483200</td>\n      <td>0.959941</td>\n      <td>0.720714</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.695100</td>\n      <td>0.959545</td>\n      <td>0.722476</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.668100</td>\n      <td>0.939976</td>\n      <td>0.721524</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.383300</td>\n      <td>0.953068</td>\n      <td>0.722381</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.684500</td>\n      <td>0.962518</td>\n      <td>0.721714</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.513900</td>\n      <td>0.956727</td>\n      <td>0.722857</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.454900</td>\n      <td>0.957581</td>\n      <td>0.722476</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=31500, training_loss=0.7127068643305037, metrics={'train_runtime': 10119.4827, 'train_samples_per_second': 24.902, 'train_steps_per_second': 3.113, 'total_flos': 8346190261248000.0, 'train_loss': 0.7127068643305037, 'epoch': 3.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"trainer.save_model('mental-health-classifier')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T11:18:03.853583Z","iopub.execute_input":"2025-06-05T11:18:03.853861Z","iopub.status.idle":"2025-06-05T11:18:04.414120Z","shell.execute_reply.started":"2025-06-05T11:18:03.853841Z","shell.execute_reply":"2025-06-05T11:18:04.413477Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Now evaluate on the test (or validation) set:\nresults = trainer.evaluate()\n\nprint(results)  # Show metrics like accuracy, eval_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T11:20:44.396872Z","iopub.execute_input":"2025-06-05T11:20:44.397389Z","iopub.status.idle":"2025-06-05T11:22:29.026593Z","shell.execute_reply.started":"2025-06-05T11:20:44.397369Z","shell.execute_reply":"2025-06-05T11:22:29.026032Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2625/2625 01:44]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.9575809240341187, 'eval_accuracy': 0.7224761904761905, 'eval_runtime': 104.6215, 'eval_samples_per_second': 200.724, 'eval_steps_per_second': 25.09, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":18}]}